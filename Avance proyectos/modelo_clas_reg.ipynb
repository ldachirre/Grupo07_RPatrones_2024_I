{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldani\\AppData\\Local\\Temp\\ipykernel_4220\\2525691506.py:3: DtypeWarning: Columns (12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep=';')\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos completos\n",
    "file_path = r'C:\\Users\\ldani\\Documents\\Patronus\\Project\\simulated-obstructive-disease-respiratory-pressure-and-flow-1.0.0\\merged_dataset_balanced.csv'\n",
    "df = pd.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis EDA del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes por columna:\n",
      "Subject Number                       0\n",
      "Age                                  0\n",
      "Gender                               0\n",
      "Height [cm]                          0\n",
      "Weight [kg]                          0\n",
      "PEEP                                 0\n",
      "COPD                                 0\n",
      "Time [s]                             0\n",
      "Pressure [cmH2O]                     0\n",
      "Flow [L/s]                           0\n",
      "V_tidal [L]                          0\n",
      "History of Smoking (yes/no)          0\n",
      "Smoking Frequency               766080\n",
      "History of vaping (yes/no)           0\n",
      "Frequency of vaping             685440\n",
      "Asthma (yes/no and severity)         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Realizar análisis de valores faltantes\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject Number                    int64\n",
      "Age                               int64\n",
      "Gender                           object\n",
      "Height [cm]                      object\n",
      "Weight [kg]                       int64\n",
      "PEEP                              int64\n",
      "COPD                              int64\n",
      "Time [s]                        float64\n",
      "Pressure [cmH2O]                float64\n",
      "Flow [L/s]                      float64\n",
      "V_tidal [L]                     float64\n",
      "History of Smoking (yes/no)      object\n",
      "Smoking Frequency                object\n",
      "History of vaping (yes/no)       object\n",
      "Frequency of vaping              object\n",
      "Asthma (yes/no and severity)     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verificar tipos de datos\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldani\\AppData\\Local\\Temp\\ipykernel_4220\\1507219382.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[num_cols] = df[num_cols].applymap(convert_range_to_mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject Number                    int64\n",
      "Age                               int64\n",
      "Gender                           object\n",
      "Height [cm]                     float64\n",
      "Weight [kg]                       int64\n",
      "PEEP                              int64\n",
      "COPD                              int64\n",
      "Time [s]                        float64\n",
      "Pressure [cmH2O]                float64\n",
      "Flow [L/s]                      float64\n",
      "V_tidal [L]                     float64\n",
      "History of Smoking (yes/no)      object\n",
      "Smoking Frequency                object\n",
      "History of vaping (yes/no)       object\n",
      "Frequency of vaping              object\n",
      "Asthma (yes/no and severity)     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Función para convertir rangos a su valor medio\n",
    "def convert_range_to_mean(value):\n",
    "    if isinstance(value, str) and '-' in value:\n",
    "        start, end = value.split('-')\n",
    "        return (float(start) + float(end)) / 2\n",
    "    return value\n",
    "\n",
    "# Aplicar la función a las columnas numéricas\n",
    "num_cols = ['Height [cm]']\n",
    "df[num_cols] = df[num_cols].applymap(convert_range_to_mean)\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject Number  Age  Gender  Height [cm]  Weight [kg]  PEEP  COPD  \\\n",
      "0               1   24  Female        164.5           65     0     0   \n",
      "1               1   24  Female        164.5           65     0     0   \n",
      "2               1   24  Female        164.5           65     0     0   \n",
      "3               1   24  Female        164.5           65     0     0   \n",
      "4               1   24  Female        164.5           65     0     0   \n",
      "\n",
      "   Time [s]  Pressure [cmH2O]  Flow [L/s]  V_tidal [L]  \\\n",
      "0      0.00          2.681998   -0.577100     0.000000   \n",
      "1      0.01          2.617630   -0.597392    -0.005872   \n",
      "2      0.02          2.681998   -0.597392    -0.011846   \n",
      "3      0.03          2.617630   -0.626598    -0.017966   \n",
      "4      0.04          2.649814   -0.626598    -0.024232   \n",
      "\n",
      "  History of Smoking (yes/no) Smoking Frequency History of vaping (yes/no)  \\\n",
      "0                          No             Never                         No   \n",
      "1                          No             Never                         No   \n",
      "2                          No             Never                         No   \n",
      "3                          No             Never                         No   \n",
      "4                          No             Never                         No   \n",
      "\n",
      "  Frequency of vaping Asthma (yes/no and severity)  \n",
      "0               Never                          No   \n",
      "1               Never                          No   \n",
      "2               Never                          No   \n",
      "3               Never                          No   \n",
      "4               Never                          No   \n",
      "Valores faltantes por columna:\n",
      "Subject Number                  0\n",
      "Age                             0\n",
      "Gender                          0\n",
      "Height [cm]                     0\n",
      "Weight [kg]                     0\n",
      "PEEP                            0\n",
      "COPD                            0\n",
      "Time [s]                        0\n",
      "Pressure [cmH2O]                0\n",
      "Flow [L/s]                      0\n",
      "V_tidal [L]                     0\n",
      "History of Smoking (yes/no)     0\n",
      "Smoking Frequency               0\n",
      "History of vaping (yes/no)      0\n",
      "Frequency of vaping             0\n",
      "Asthma (yes/no and severity)    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rellenar los espacios en blanco en las columnas especificadas\n",
    "df['Smoking Frequency'] = df['Smoking Frequency'].fillna('Never')\n",
    "df['Frequency of vaping'] = df['Frequency of vaping'].fillna('Never')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Realizar análisis de valores faltantes\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las columnas no tengan espacios adicionales\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Crear una columna binaria para indicar si hay EPOC o no\n",
    "df['has_EPOC'] = df['COPD'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas categóricas a numéricas usando OneHotEncoder\n",
    "categorical_features = ['Gender', 'History of Smoking (yes/no)', 'History of vaping (yes/no)', 'Asthma (yes/no and severity)']\n",
    "numerical_features = ['Age', 'Height [cm]', 'Weight [kg]', 'PEEP', 'Time [s]', 'Pressure [cmH2O]', 'Flow [L/s]', 'V_tidal [L]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el transformador de columnas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar las características\n",
    "X = df.drop(columns=['Subject Number', 'COPD', 'has_EPOC'])\n",
    "y_classification = df['has_EPOC']\n",
    "y_regression = df['COPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el preprocesamiento\n",
    "X_preprocessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el resultado preprocesado a un DataFrame\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir las columnas de 'Subject Number' y 'has_EPOC' al DataFrame preprocesado\n",
    "X_preprocessed_df['Subject Number'] = df['Subject Number'].values\n",
    "X_preprocessed_df['has_EPOC'] = y_classification.values\n",
    "X_preprocessed_df['COPD'] = y_regression.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba con estratificación por sujeto\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Obtener los índices para el conjunto de entrenamiento y prueba\n",
    "train_idx, test_idx = next(gss.split(X_preprocessed_df, groups=X_preprocessed_df['Subject Number']))\n",
    "\n",
    "train_data = X_preprocessed_df.iloc[train_idx]\n",
    "test_data = X_preprocessed_df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los datos en archivos CSV temporales para que AutoGluon pueda cargarlos\n",
    "train_data.to_csv(\"train_data_classification.csv\", index=False)\n",
    "test_data.to_csv(\"test_data_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de ML se dividirá en 2 secciones:\n",
    "1. Modelo de Clasificación (EPOC: 0 & EPOC:1) siendo si el sujeto presenta o no EPOC\n",
    "2. Modelo de Regresión, nivel de EPOC presente en el sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240624_013432\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Loaded data from: train_data_classification.csv | Columns = 22 / 22 | Rows = 645120 -> 645120\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240624_013432\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          12\n",
      "Memory Avail:       7.14 GB / 15.95 GB (44.7%)\n",
      "Disk Space Avail:   501.05 GB / 930.89 GB (53.8%)\n",
      "===================================================\n",
      "Train Data Rows:    645120\n",
      "Train Data Columns: 21\n",
      "Label Column:       has_EPOC\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7305.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 103.36 MB (1.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 11 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['cat__History of vaping (yes/no)_No ', 'cat__Asthma (yes/no and severity)_Yes - Moderate']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['cat__History of vaping (yes/no)_No ', 'cat__Asthma (yes/no and severity)_Yes - Moderate']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 17 | ['num__Age', 'num__Height [cm]', 'num__Weight [kg]', 'num__PEEP', 'num__Time [s]', ...]\n",
      "\t\t('int', [])   :  2 | ['Subject Number', 'COPD']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 8 | ['num__Age', 'num__Height [cm]', 'num__Weight [kg]', 'num__PEEP', 'num__Time [s]', ...]\n",
      "\t\t('int', [])       : 2 | ['Subject Number', 'COPD']\n",
      "\t\t('int', ['bool']) : 9 | ['cat__Gender_Female', 'cat__Gender_Male', 'cat__History of Smoking (yes/no)_No', 'cat__History of Smoking (yes/no)_No ', 'cat__History of Smoking (yes/no)_Yes', ...]\n",
      "\t2.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.76 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.38s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 638668, Val Rows: 6452\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3597.62s of the 3597.62s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t10.03s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3587.32s of the 3587.32s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t6.88s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3580.18s of the 3580.18s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3578.75s of the 3578.74s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3577.6s of the 3577.6s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t51.48s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3526.0s of the 3526.0s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t51.97s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3473.91s of the 3473.91s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3471.26s of the 3471.26s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t48.86s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3422.25s of the 3422.25s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t41.21s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3380.92s of the 3380.92s of remaining time.\n",
      "No improvement since epoch 0: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t350.86s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3029.69s of the 3029.69s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3027.7s of the 3027.69s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t361.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2666.17s of the 2666.17s of remaining time.\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2663.95s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 936.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240624_013432\")\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo de clasificación con AutoGluon\n",
    "predictor_classification = TabularPredictor(label='has_EPOC', problem_type='binary').fit(\n",
    "    train_data=\"train_data_classification.csv\",\n",
    "    time_limit=3600  # Puedes ajustar el límite de tiempo según sea necesario\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set (classification):\n",
      " 0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "161275    1\n",
      "161276    1\n",
      "161277    1\n",
      "161278    1\n",
      "161279    1\n",
      "Name: has_EPOC, Length: 161280, dtype: int64\n",
      "Performance on test set (classification):\n",
      " {'accuracy': 0.9961991567460318, 'balanced_accuracy': 0.9923983134920635, 'mcc': 0.9898641563428314, 'roc_auc': 1.0, 'f1': 0.9974725088956967, 'precision': 0.9949577620030764, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Predecir en el conjunto de prueba\n",
    "test_data_classification = pd.read_csv('test_data_classification.csv')\n",
    "classification_predictions = predictor_classification.predict(test_data_classification)\n",
    "print(\"Predictions on test set (classification):\\n\", classification_predictions)\n",
    "\n",
    "# Evaluar el modelo de clasificación\n",
    "classification_performance = predictor_classification.evaluate(test_data_classification)\n",
    "print(\"Performance on test set (classification):\\n\", classification_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir las predicciones al conjunto de prueba para filtrar los que tienen EPOC\n",
    "test_data_classification['has_EPOC'] = classification_predictions\n",
    "\n",
    "# Filtrar los datos con EPOC para la regresión\n",
    "train_data_with_EPOC = train_data[train_data['has_EPOC'] == 1]\n",
    "test_data_with_EPOC = test_data_classification[test_data_classification['has_EPOC'] == 1]\n",
    "\n",
    "# Guardar los conjuntos con EPOC en archivos CSV\n",
    "train_data_with_EPOC.to_csv('train_data_regression.csv', index=False)\n",
    "test_data_with_EPOC.to_csv('test_data_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos los mejores hiperparámetros y modelos de la prueba de Autogluon en entregables anteriores para regresión\n",
    "hyperparameters = {\n",
    "    'GBM': ['GBMLarge'],\n",
    "    'KNN': [{'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}]\n",
    "}\n",
    "\n",
    "# Configurar el ensamble con los pesos específicos\n",
    "ensemble_kwargs = {\n",
    "    'weights': {'GBMLarge': 0.786, 'KNeighborsDist': 0.214}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el modelo de regresión para volver a entrenar\n",
    "path_predictor_regression = rf\"C:\\Users\\ldani\\Documents\\Patronus\\Project\\AutogluonModels\\ag-20240612_190315\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\ldani\\Documents\\Patronus\\Project\\AutogluonModels\\ag-20240612_190315\"\n",
      "Presets specified: ['best_quality']\n",
      "Loaded data from: train_data_regression.csv | Columns = 22 / 22 | Rows = 483840 -> 483840\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 3600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: C:\\Users\\ldani\\Documents\\Patronus\\Project\\AutogluonModels\\ag-20240612_190315\\ds_sub_fit\\sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "2024-06-23 21:18:17,655\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_sub_fit()\u001b[39m (pid=1632, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py\", line 4997, in _sub_fit\n",
      "    predictor._fit(ag_fit_kwargs=ag_fit_kwargs, ag_post_fit_kwargs=ag_post_fit_kwargs)\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py\", line 1142, in _fit\n",
      "    self._learner.fit(**ag_fit_kwargs)\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py\", line 159, in fit\n",
      "    return self._fit(X=X, X_val=X_val, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py\", line 128, in _fit\n",
      "    trainer.fit(\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py\", line 125, in fit\n",
      "    self._train_multi_and_ensemble(\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2590, in _train_multi_and_ensemble\n",
      "    model_names_fit = self.train_multi_levels(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 452, in train_multi_levels\n",
      "    base_model_names, aux_models = self.stack_new_level(\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 620, in stack_new_level\n",
      "    aux_models += self._stack_new_level_aux(X_val, y_val, X, y, all_base_model_names, level, infer_limit, infer_limit_batch_size, **full_aux_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 746, in _stack_new_level_aux\n",
      "    aux_models = self.stack_new_level_aux(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 803, in stack_new_level_aux\n",
      "    X_stack_preds = self.get_inputs_to_stacker(X, base_models=base_model_names, fit=fit, use_orig_features=False, use_val_cache=use_val_cache)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1287, in get_inputs_to_stacker\n",
      "    X_stacker = convert_pred_probas_to_df(pred_proba_list=pred_proba_list, problem_type=self.problem_type, columns=stack_column_names, index=X.index)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ldani\\Documents\\Patronus\\Project\\venv\\Lib\\site-packages\\autogluon\\core\\utils\\utils.py\", line 328, in convert_pred_probas_to_df\n",
      "    pred_proba_list = np.concatenate(pred_proba_list, axis=1)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo de regresión con AutoGluon\n",
    "predictor_regression = TabularPredictor(label='COPD', problem_type='regression',path=path_predictor_regression).fit(\n",
    "    train_data='train_data_regression.csv',\n",
    "    presets='best_quality', # Presets para asegurar la mejor calidad de modelos\n",
    "    hyperparameters=hyperparameters,\n",
    "    time_limit=3600  # Puedes ajustar el límite de tiempo según sea necesario\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de prueba\n",
    "test_data_regression = pd.read_csv('test_data_regression.csv')\n",
    "regression_predictions = predictor_regression.predict(test_data_regression)\n",
    "\n",
    "# Evaluar el modelo de regresión\n",
    "rmse = mean_squared_error(test_data_regression['COPD'], regression_predictions, squared=False)\n",
    "print(\"RMSE on test set (regression):\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
